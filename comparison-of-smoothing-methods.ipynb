{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Comparison of Basis Expansions in Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Beginners in machine learning often write off regression methods after learning of more exotic algorithms like Boosting, Random Forests, and Support Vector machines; why bother with a *linear* method when powerful non-parametric methods are readily avalable?\n",
    "\n",
    "It's easy to point out that the *linear* in linear regression is not meant to convey that the resulting model predictions are linear in the raw *feaure*, just the estimated parameters!  The modeler can certainly capture non-linearities in thier regression, they only need to transform the raw predictors!\n",
    "\n",
    "A common response is that it is error prone and  annoying work to explore data by hand and somehow divine correct transformations of predictors: other methods do it automatically.\n",
    "\n",
    "Ususally the only truly flexible method beginners learn to capture non-linearities in regression is polynomial regression, which is a real shame, as it is about the worst performing method available.\n",
    "\n",
    "The purpose of this post is to spread awareness of better options for capturing non-lineararities in regression models, we would like to advocate more widespread adoption of linear and cubic splines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Acknowledgements\n",
    "\n",
    "The idea for this post is based on my answer to [hxd1011](https://stats.stackexchange.com/users/113777/hxd1011)s [question regarding grouping vs. splines](https://stats.stackexchange.com/questions/230750/when-should-we-discretize-bin-continuous-independent-variables-features-and-when) at CrossValidated.\n",
    "\n",
    "### Software\n",
    "\n",
    "I've taken the oppurtunity to write a small [python module](https://github.com/madrury/basis-expansions) that is useful for using the basis expansions in this post here.  It conforms to the sklearn transfrmation interface, so can be used in pipelines and other high level processes in sklearn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basis Expansions in Regression\n",
    "\n",
    "To capture non-linearities in regression models, we need to transform some or all of the predictors.  To avoid having to treat every predictor as a special case needing investigation, we would like some way of applying a very general *family* of transformations to our predictors, which is flexible enough to adapt (when the model is fit) to a wide variety of shapes.\n",
    "\n",
    "This takes the general form of a *basis expansion*.  Basis here is used in the linear algebraic sense: a linearly independent set of objects.  In this case our objects are *functions*:\n",
    "\n",
    "$$ B = f_1, f_2, \\ldots, f_k $$\n",
    "\n",
    "and we create new sets of features by applying every function in our basis to the given feature:\n",
    "\n",
    "$$ f_1(x), \\ f_2(x), \\ \\ldots, \\ f_k(x) $$\n",
    "\n",
    "\n",
    "### Polynomial Expansion\n",
    "\n",
    "The most commonly, and often *only*, example taught in introductory modeling courses or textbooks is **polynomial regression**.\n",
    "\n",
    "In polynomial regression we choose as our basis a set of polynomial terms of increasing degree:\n",
    "\n",
    "$$ f_1(x) = x, \\ f_2(x) = x^2, \\ \\ldots, \\ f_k(x) = x^k $$\n",
    "\n",
    "This allows us to fit *polynomial* curves to features:\n",
    "\n",
    "![Polynomials of Various Degrees](img/polynomial-various-degrees.png)\n",
    "\n",
    "Unfortunately, polynomial regression has a fair number of issues.  The most often observed is a very high varaince (sensitivity to data), especially near the boundaries of the data:\n",
    "\n",
    "![Polynomial Regressions on One Plot](img/polynomial-regressions-one-plot.png)\n",
    "\n",
    "Above we have a fixed data set, and we have fit and plotted polynomial regressions of various degrees.  The most striking feature is how badly the higher degree polynomials fit the data near the edges.  The variance explodes!  This is especially problamatic in high dimensional situations where almost *all* of the data is near the boundaries, due to the curse of dimensionality.\n",
    "\n",
    "Another way to look at this is to plot residuals for each data point $x$ over many samples from the same population, as we vary the degree of the polynomial we fit to the data:\n",
    "\n",
    "![Residuals from Polynomial Regression](img/polynomial-residuals-various-degrees.png)\n",
    "\n",
    "Here we see the same pattern from earlier, the instability in our fits starts at the edges of the data, and moves inward as we increase the degree.\n",
    "\n",
    "Another final way to observe this effect is to estimate the average testing error of polynomial regressions fit repretedly to the same population as the degree is changed:\n",
    "\n",
    "![Polynomail Regression Average Error by Degree](img/polynomial-train-test-weird-signal.png)\n",
    "\n",
    "The polynomial regression eventually drasticly overfits, even to this simple one dimensional data set.\n",
    "\n",
    "There are other issues with polynomial regression; for example, it is inherently non-local, changing the value of $y$ at one point in the training set can affect the fit of the polynomial at data points very far away, resulting in tight coupling across the space of our data (often referred to as *rigidity*).  You can get a feel for this by playing around with the interactive [scatterplot somoothers](http://madrury.github.io/smoothers/) app hoseted on this site.\n",
    "\n",
    "The methods we will lay out in the rest of this post will go some way to alleviate these issues with polynomial regression, and serve as superior solutions to the same underlying problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binning Expansion\n",
    "\n",
    "Probably the first thing that occurs to most modelers when reflecting on other ways to capture non-linear effects in regression is to **bin** the predictor varaible:\n",
    "\n",
    "![Regression with Bins, Various Number of Cuts](img/bins-various-n-cuts.png)\n",
    "\n",
    "In binned regression we simply cut the range of the predictor varaible into equally sized intervals (though we could use a more sophisticated rule, like cutting into intervals at percentiles of the marginal distribution of the predictor).  Membership in any interval is used to create a set of indicator variables, which are then regressed upon.  In the one predictor case, this results in our regression predicting the mean value of $y$ in each bin.\n",
    "\n",
    "Binning has its obvious [conceptual issues](https://stats.stackexchange.com/questions/68834/what-is-the-benefit-of-breaking-up-a-continuous-predictor-variable).  Most prominently, we expect most phenomina we study to vary continuously with inputs.  Binned regression does not create continuous functions of the predictor, so in most cases we would expect there to be some unavoidable bias within each bin.  In the simple case where the true relationship is monotonic in an interval, we expect to be underpredicting the truth on the left hand side of each bin, and on the right hand side we are expect to overpredict.\n",
    "\n",
    "Even so, binning is popular.  It is easy to discover, implement, and explain, and often does a good enough job of capturing the non-linear behaviour of the predictor response relationship.  There are better options though, we will see later that other methods are both more cenceptually appealing, and do a better job of capturing the predictive power in the data with less estiamted parameters."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
